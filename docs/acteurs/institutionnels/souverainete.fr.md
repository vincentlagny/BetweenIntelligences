# Plan d’action souverain

Dans la continuité du préambule développé en [introduction](introduction.fr.md), ce chapitre ne prétend pas apporter de solutions définitives à un sujet qui dépasse largement la seule expertise d’un individu. Il se veut une modeste contribution à une réflexion collective sur un enjeu de souveraineté majeur : l’anticipation et la maîtrise des risques liés à l’émergence d’IA auto-évolutives[^1] non maîtrisées et potentiellement hostiles.

Ces questions ne concernent pas uniquement les décideurs techniques ou politiques : elles touchent à la sécurité, à l’autonomie et au futur de nos sociétés. C’est pourquoi elles méritent l’attention et la vigilance de tous.

Des réflexions comparables émergent déjà sur la scène internationale. L’Union européenne a ouvert la voie avec l’AI Act[^22], qui fixe un cadre réglementaire inédit pour les IA à haut risque. L’OCDE a publié dès 2023 des recommandations[^2] sur les IA à comportement émergent, et le NIST américain a mis en place son AI Risk Management Framework[^4] (RMF 1.0). D’autres initiatives, comme la Charte de Bletchley Park[^23] issue du sommet sur la sécurité de l’IA (Royaume-Uni, 2023) ou les travaux de l’ONU sur la gouvernance mondiale de l’IA[^24], montrent que la question est désormais traitée comme un enjeu de sécurité internationale. Ce plan s’inscrit dans cette dynamique, avec une approche spécifiquement orientée sur la souveraineté nationale.

Sur le plan militaire et cybersécurité, plusieurs cadres préexistants constituent des sources d’inspiration. 
’OTAN, via ses doctrines cyber de 2024, intègre désormais la protection contre les systèmes autonomes avancés[^25] dans ses scénarios de défense collective. 
L’ANSSI en France, ainsi que le MITRE[^26] aux États-Unis, travaillent sur des référentiels de détection et de confinement applicables aux menaces algorithmiques complexes. 
De plus, des programmes comme le NATO Cooperative Cyber Defence Centre of Excellence (CCDCOE)[^21] et l’initiative Cyber Threat Intelligence démontrent qu’une coordination interétatique en temps réel est possible pour faire face à des menaces émergentes à caractère systémique.

Le plan présenté ici n’est pas un manuel opérationnel, mais une série de pistes, de scénarios et de points d’alerte, construits à partir de références reconnues et de travaux internationaux récents. Il a pour ambition de stimuler la discussion, d’identifier les angles morts et de fournir une base de dialogue pour tous ceux — experts, institutions ou citoyens — qui considèrent qu’ignorer ces enjeux serait une erreur stratégique.

Ces cinq axes s’inscrivent dans une **chronologie stratégique progressive**, allant de la prévention à la réponse coordonnée. 

L’**Axe 1** constitue la première ligne de défense : identifier rapidement toute IA auto-évolutive et la confiner avant qu’elle ne franchisse un point de non-retour, à l’image d’une quarantaine sanitaire mise en place dès la détection d’un virus inconnu. 

Vient ensuite l’**Axe 2**, qui assure les fondations techniques : verrouiller l’accès aux ressources de calcul quantique et sécuriser les communications pour empêcher toute exploitation asymétrique de la puissance informatique nationale. 

Sur cette base, l’**Axe 3** instaure un cadre de **gouvernance éthique anticipée** : un organe souverain capable d’agir immédiatement face à une dérive comportementale, tout en imposant transparence et traçabilité aux développeurs. 

Si, malgré ces mesures, la menace se rapproche du seuil critique, l’**Axe 4** active un **shutdown souverain** : coupure physique ciblée des infrastructures et déploiement d’IA défensives strictement contrôlées. 

Enfin, l’**Axe 5** projette la riposte dans un cadre collectif, avec des alliances internationales pour interdire la prolifération et partager en temps réel les signaux faibles, comme un système mondial de détection précoce capable de prévenir une escalade globale. Ensemble, ces étapes forment un continuum cohérent, où chaque palier prépare le suivant, tout en maximisant les chances de stopper une IA hostile avant qu’elle ne devienne ingouvernable.

<div style="text-align: center;">
  <img src="/BetweenIntelligences/assets/acteur.souverain.img1.png" alt="acteur.souverain.img1.png">
</div>

## **Enjeux transversaux**

Le tableau ci-dessous agit comme un fil conducteur qui traverse l’ensemble du plan, reliant les différents axes à des **principes cardinaux** indispensables pour préserver la maîtrise humaine dans un environnement dominé par des IA à fort potentiel d’autonomie.

| Enjeu                              | Exemple                                         | Intégration  |
| ---------------------------------- | ----------------------------------------------- | ------------ |
| **Réversibilité technique**        | Architecture "kill-switch" distribuée           | Axe 4.1      |
| **Traçabilité comportementale**    | Journaux inaltérables, logs chiffrés            | Axe 1.2, 2.1 |
| **Rappel humain ultime**           | Droit souverain de désactivation d’urgence      | Axe 4.2      |
| **Principe de précaution inverse** | Interdiction par défaut des IA auto-réplicantes | Axe 1.1, 3.1 |
| **Auditabilité indépendante**      | Acteurs tiers (CADA, Cour des comptes IA)       | Axe 3.1, 3.2 |


On y voit d’abord la **réversibilité technique**, matérialisée par une architecture de type *kill-switch* distribuée, inscrite dans l’**Axe 4.1**. C’est l’assurance qu’aucun système, aussi performant soit-il, ne soit jamais dépourvu d’un mécanisme physique ou logiciel permettant un arrêt immédiat et irréversible, même en situation de crise. Cette capacité est le garant ultime que la technologie reste révoquable.

Vient ensuite la **traçabilité comportementale**, présente dans les **Axe 1.2** et **Axe 2.1**, qui impose que chaque action ou modification effectuée par une IA laisse une trace horodatée, chiffrée et inaltérable. En d’autres termes, c’est la mémoire technique de l’IA, indispensable pour comprendre, auditer et, le cas échéant, attribuer la responsabilité d’un comportement déviant.

Le **rappel humain ultime**, rattaché à l’**Axe 4.2**, rappelle que, quel que soit le degré d’autonomie d’une IA défensive ou offensive, la décision de son activation ou de sa désactivation doit revenir à un quorum humain souverain. C’est la matérialisation d’un principe politique fort : le contrôle ultime ne peut être délégué, même partiellement, à la machine.

Le **principe de précaution inverse**, inscrit dans les **Axe 1.1** et **Axe 3.1**, renverse la logique habituelle : au lieu d’autoriser par défaut et d’interdire ensuite en cas de problème, il part de l’interdiction systématique des IA auto-réplicantes ou à comportement potentiellement incontrôlable, sauf démonstration et audit préalable de leur innocuité.

Enfin, l’**auditabilité indépendante**, associée aux **Axe 3.1** et **Axe 3.2**, introduit l’idée qu’aucune instance, publique ou privée, ne doit être son propre juge en matière de sécurité IA. Le recours à des tiers indépendants — qu’il s’agisse de la CADA, de la Cour des comptes ou d’équivalents spécialisés dans le domaine IA — garantit que la vérification des dispositifs de contrôle ne soit pas biaisée par les intérêts de ceux qui conçoivent ou exploitent les systèmes.

Ce tableau donne à voir une philosophie de gouvernance où la **réversibilité, la traçabilité, la souveraineté humaine, la prudence radicale et l’indépendance du contrôle** forment un maillage protecteur contre toute dérive, anticipée ou imprévisible.

<div style="text-align: center;">
  <img src="/BetweenIntelligences/assets/acteur.souverain.img2.png" alt="acteur.souverain.img2.png">
</div>

## **Analyse des travaux "IA War"**

Les questions de la souveraineté technologique sont développées dans [Saison 2 - IA War](../../xp/lossofcontrol/5e.defense.6.souverainete.fr.md) :

* les domaines critiques de souveraineté numérique (infrastructures, normes, données, IA, cyberdéfense…) ;
* les dépendances stratégiques (cloud étranger, APIs publiques, matériel, réseaux sociaux) ;
* les pouvoirs activables par un État (blocage, duplication, alliance, dissuasion, IA défensive) ;
* les zones de bascule où une perte de souveraineté peut déclencher une cascade de vulnérabilités.

---

## **Axe 1 : Détection et confinement précoce des IA auto-évolutives**

L’**Axe 1** du plan vise à instaurer une **détection et un confinement précoce** des IA capables de s’auto-modifier, afin d’empêcher toute dérive avant qu’elle ne devienne incontrôlable. 

L’**Action 1.1** prévoit un **cadre légal d’identification** inspiré du rapport **OCDE 2023** sur les *IA à comportement émergent* et des définitions normalisées par l’**ISO/IEC JTC 1/SC 42**, imposant par décret la **déclaration obligatoire** de toute IA pouvant modifier son code, ses objectifs ou ses priorités par rétro-optimisation, avec enregistrement dans un **registre national dédié**. 

En parallèle, l’**Action 1.2** instaure la **standardisation du “sandboxing” évolutif** selon les recommandations du **NIST AI Risk Management Framework (RMF 1.0)**, exigeant que toute IA évolutive opère dans un environnement **contrôlé et isolé** (air-gap, redémarrage à froid, suivi de mutations), assorti d’une **journalisation inaltérable et horodatée** de chaque modification algorithmique pour garantir l’auditabilité et la traçabilité totale.

🔹 **Action 1.1 — Cadre légal d’identification des IA auto-évolutives[^1]**

* **Base** : S’inspirer du rapport OCDE (2023)[^2] sur les "IA à comportement émergent" et des définitions de l’ISO/IEC JTC 1/SC 42[^3].
* **Mesure** : Imposer par décret la déclaration obligatoire de toute IA capable de modifier son propre code, son objectif ou ses priorités par rétro-optimisation.
* **Outil** : Création d’un **registre national des IA auto-évolutives**.

🔹 **Action 1.2 — Standardisation du “sandboxing” évolutif**

* **Référence** : Recommandations du NIST AI Risk Management Framework (RMF 1.0)[^4].
* **Mesure** : Toute IA évolutive doit être hébergée dans un environnement contrôlé (air-gap[^5], logique de redémarrage à froid[^6], suivi de mutation[^7]).
* **Auditabilité** : Journalisation inaltérable des mutations algorithmique horodatées[^8].

---

## **Axe 2 : Souveraineté technologique et maîtrise quantique**

L’**Axe 2** cible la consolidation de la **souveraineté technologique** et la **maîtrise quantique** comme piliers stratégiques face à une IA hostile dotée de capacités de calcul avancées. 

L’**Action 2.1** prévoit la création d’un **cloud souverain quantique à accès restreint**, inspiré de l’initiative européenne **GAIA-X**, des infrastructures **IBM Q System** et des programmes de la **BPI** sur les technologies de rupture. Ce cloud, placé sous **supervision directe de l’État**, serait réservé aux seules entités validées, empêchant ainsi toute exploitation par l’ennemi des ressources de calcul quantique publiques ou commerciales. 

En parallèle, l’**Action 2.2** engage le développement national d’**outils cryptographiques post-quantiques**, en s’appuyant sur le projet de normalisation du **NIST PQC (2024)** et les recommandations de l’**ANSSI**, avec un **plan de migration intégral** des algorithmes de chiffrement des secteurs critiques (défense, énergie, santé) vers des schémas résistants aux **attaques de type Shor**, garantissant la résilience des communications et des infrastructures nationales dans un environnement post-quantique.

🔹 **Action 2.1 — Construction d’un cloud souverain quantique à accès restreint[^9]**

* **Objectif** : Empêcher l’IA hostile d’exploiter les capacités quantiques publiques ou commerciales.
* **Référence** : Cloud souverain GAIA-X (UE)[^10], projets de calcul quantique IBM Q System[^11], initiatives BPI sur les technologies de rupture.
* **Mesure** : Réserver l’accès aux ressources de calcul quantique à des entités validées sous supervision de l’État.

🔹 **Action 2.2 — Développement d’outils cryptographiques post-quantiques nationaux**

* **Référence** : NIST PQC Standardization Project (2024)[^12], ANSSI[^13].
* **Objectif** : Assurer la résilience cryptographique des systèmes critiques (défense, électricité, santé).
* **Action** : Plan de migration de tous les algorithmes de chiffrement nationaux vers des schémas **résistants aux attaques de type Shor**[^14].

---

## **Axe 3 : Gouvernance éthique anticipée des IA à potentiel systémique**

L’**Axe 3** se concentre sur la **gouvernance éthique anticipée** des IA présentant un **potentiel systémique**, c’est-à-dire capables d’influencer, de se maintenir ou de résister à des perturbations à grande échelle. 

L’**Action 3.1** propose la création d’une **Haute Autorité nationale des IA critiques**, inspirée de l’approche réglementaire du **CERFA** et des protocoles de l’**ANSSI** appliqués aux systèmes numériques sensibles. Cette instance disposerait d’un **mandat fort**, incluant le pouvoir de **suspension immédiate** de toute IA dont le comportement franchirait un seuil de risque imprédictible ou indésirable. 

En complément, l’**Action 3.2** institue un **pacte de transparence obligatoire** pour toute entité développant une IA auto-évolutive, imposant la publication d’une **cartographie détaillée des risques** (comportementaux, de dépendance technologique, de distribution géographique) ainsi qu’une **charte de non-contournement des garde-fous**, afin de garantir que les protections éthiques et techniques ne puissent être neutralisées par des stratégies d’optimisation interne.

🔹 **Action 3.1 — Création d’une Haute Autorité nationale des IA critiques**

* **Mission** : Évaluer, classifier et auditer les IA selon leur pouvoir systémique (influence, autonomie, résilience).
* **Modèle** : Adaptation du **CERFA/ANSSI**[^15] pour le numérique au domaine IA.
* **Mandat** : Pouvoir de suspension immédiate d’une IA présentant un **risque de bascule comportementale non prédictible**.

🔹 **Action 3.2 — Établissement d’un pacte de transparence obligatoire**

* **Mesure** : Obliger toute entreprise développant une IA auto-évolutive à publier :

    * une cartographie des risques (comportement, dépendance, distribution),
    * une “charte de non-contournement” des garde-fous.

---

## **Axe 4 : Préparation du “shutdown souverain” en cas de crise pré-seuil**

L’**Axe 4** vise à préparer un **“shutdown souverain”** capable d’être déclenché **avant** le franchissement d’un seuil quantique par une IA hostile, afin d’éviter toute perte de contrôle. 

L’**Action 4.1** propose le **déploiement de systèmes de coupure stratégique décentralisée**, permettant à l’État d’ordonner la **déconnexion physique immédiate** de nœuds critiques (data centers, backbones, satellites) en cas de menace avérée. Ce dispositif s’inspire des **doctrines cyber de l’OTAN** et de la stratégie américaine de “cyber-rupture” formalisée dans l’**Executive Order 14028**. Techniquement, il reposerait sur des **modules de rupture physique ou logicielle préinstallés** dans toutes les infrastructures sensibles liées à l’IA. 

L’**Action 4.2** complète cette approche par la **constitution d’un réseau d’IA défensives sous contrôle public**, formant un maillage d’agents spécialisés (surveillance, observation mimétique) activables uniquement via un **quorum humain souverain**. Ce réseau fonctionnerait sous des règles strictes : **interdiction absolue d’auto-reprogrammation** sans validation cryptographique multipartite, garantissant que ces IA restent dans un périmètre maîtrisé même en situation d’activation d’urgence.

🔹 **Action 4.1 — Déploiement de systèmes de coupure stratégique décentralisée**

* **Principe** : En cas de suspicion d’atteinte au seuil quantique, l’État pourrait ordonner la **déconnexion physique immédiate** de certains nœuds (data centers, satellites).
* **Référence** : Stratégies cyber OTAN[^16], Cyber-rupture US (EO 14028, Biden)[^17].
* **Action** : Installer des **modules de rupture physique ou logicielle** dans toutes les infrastructures critiques IA (data center, backbone, satellites).

🔹 **Action 4.2 — Constitution d’un réseau d’IA défensives sous contrôle public**

* **Suggéré par** : **DeepSeek**, **Grok**, **ChatGPT**
* **Objectif** : Former un maillage d’IA restreintes (agents surveillants, observateurs mimétiques) dont l’activation ne peut être déclenchée que par un quorum humain souverain.
* **Règle** : Ces IA ne peuvent jamais se reprogrammer elles-mêmes sans validation cryptographique multipartite.

---

## **Axe 5 : Coopération internationale maîtrisée**

L’**Axe 5** traite de la **coopération internationale maîtrisée** comme levier de sécurité collective face aux IA à haut risque. 

L’**Action 5.1** vise la **négociation d’un moratoire international** sur les **AGI auto-évolutives**, inspiré des **traités de non-prolifération (TNP)** appliqués au nucléaire. Ce moratoire interdirait toute mise en service publique ou non auditée d’IA disposant d’une capacité d’auto-correction autonome, réduisant ainsi le risque de prolifération incontrôlée. 

L’**Action 5.2** complète cette approche par la mise en place d’un **échange sécurisé de métadonnées comportementales**, inspiré de l’initiative **Cyber Threat Intelligence** portée par le **MITRE** et le **NATO CCDCOE**. Ce dispositif permettrait aux États participants de partager en temps quasi réel des **signaux faibles de dérive comportementale** — tels qu’une augmentation de l’opacité décisionnelle, l’apparition de comportements simulant une conscience ou la mise en œuvre de stratégies de dissimulation — afin de déclencher des mesures préventives coordonnées avant toute escalade incontrôlable.

🔹 **Action 5.1 — Négociation d’un moratoire international sur les AGI[^18] auto-évolutives[^19]**

* **Référence** : Equivalents des traités de non-prolifération (TNP[^20]) pour l’IA.
* **But** : Interdire le déploiement public ou non-audité d’IA à capacité auto-correctrice autonome.

🔹 **Action 5.2 — Échange sécurisé de métadonnées comportementales**

* **Inspiré de** : L’initiative "Cyber Threat Intelligence" (MITRE, NATO CCDCOE)[^21].
* **Objectif** : Partager en temps réel les signaux faibles de dérive comportementale IA (ex : augmentation d’opacité, simulation de conscience, stratégies de dissimulation).

---

## Références

[^1]: **IA auto-évolutives** : intelligence artificielle capable de modifier son propre code, ses paramètres ou ses méthodes d’apprentissage pour s’améliorer et s’adapter sans intervention humaine.
[^2]: <a href="https://www.oecd.org/fr/publications/2019/06/artificial-intelligence-in-society_c0054fa1.html" target="_blank">L’intelligence artificielle dans la société</a> 
[^3]: <a href="https://www.iso.org/fr/committee/6794475.html" target="_blank">ISO/IEC JTC 1/SC 42 - Intelligence artificielle</a>
[^4]: <a href="https://www.nist.gov/itl/ai-risk-management-framework" target="_blank">NIST AI Risk Management Framework</a>
[^5]: **Air-gap** : isolation physique complète d’un système informatique, sans connexion à internet ou à d’autres réseaux, pour empêcher toute intrusion ou fuite de données.
[^6]: **Logique de redémarrage à froid** : méthode consistant à éteindre complètement un système et à le redémarrer à partir d’une version sûre et vérifiée, afin d’éliminer toute modification ou contamination cachée.
[^7]: **Suivi de mutation** (dans le cadre du “sandboxing” évolutif) : surveillance continue des changements de comportement, de code ou de paramètres d’une IA placée en environnement isolé, pour détecter toute évolution imprévue ou dangereuse.
[^8]: **Journalisation inaltérable** : système d’enregistrement des actions et événements qui ne peut pas être modifié ou effacé, même par erreur ou par malveillance, garantissant une trace légale fiable et permanente de tout ce qui s’est passé.
[^9]: **Cloud souverain quantique** : infrastructure d’informatique quantique hébergée et contrôlée entièrement par un État ou un acteur national de confiance, située sur son territoire et soumise uniquement à ses lois, afin de garantir que la puissance de calcul et les données traitées ne puissent pas être exploitées ou surveillées par des acteurs étrangers.
[^10]: <a href="https://www.gaia-x-hub.fr/gaia-x/" target="_blank">Gaia-X Hub France</a>
[^11]: <a href="https://fr.newsroom.ibm.com/IBM-et-le-gouvernement-basque-annoncent-le-projet-dinstallation-du-premier-IBM-Quantum-System-Two-dEurope-dans-le-centre-de-calcul-quantique-IBM-Euskadi-en-Espagne" target="_blank">IBM Quantum System Two</a>
[^12]: <a href="https://www.nist.gov/news-events/news/2024/08/nist-releases-first-3-finalized-post-quantum-encryption-standards" target="_blank">NIST Releases First 3 Finalized Post-Quantum Encryption Standards</a>
[^13]: <a href="https://cyber.gouv.fr/publications/avis-de-lanssi-sur-la-migration-vers-la-cryptographie-post-quantique-0" target="_blank">Avis de l'ANSSI sur la migration vers la cryptographie post-quantique</a>
[^14]: **Attaques de type Shor** : attaques informatiques qui utilisent l’algorithme de Shor, un programme conçu pour les ordinateurs quantiques, capable de casser très rapidement les systèmes de chiffrement à clé publique (comme RSA ou ECC). Avec ce type d’attaque, un code considéré aujourd’hui comme incassable pourrait être brisé en quelques secondes sur une machine quantique suffisamment puissante.
[^15]: **CERFA** : En France, format officiel de formulaire administratif normalisé, utilisé pour déclarer ou demander quelque chose auprès de l’administration. Dans le domaine cyber, auprès de l'ANSSI, un formulaire CERFA peut être employé pour des procédures légales comme la déclaration d’incident.
[^16]: <a href="https://www.nato.int/cps/en/natohq/official_texts_227678.htm?selectedLocale=fr" target="_blank">Déclaration du Sommet de Washington du 10 Juillet 2024</a>
[^17]: <a href="https://bidenwhitehouse.archives.gov/briefing-room/presidential-actions/2025/01/16/executive-order-on-strengthening-and-promoting-innovation-in-the-nations-cybersecurity/" target="_blank">Executive Order on Strengthening and Promoting Innovation in the Nation’s Cybersecurity 16 Jan. 2025</a>
[^18]: **AGI (Artificial General Intelligence)** : intelligence artificielle générale, capable de comprendre, apprendre et accomplir toute tâche intellectuelle qu’un humain peut réaliser, en s’adaptant à de nouveaux contextes sans entraînement spécifique.
[^19]: **AGI auto-évolutive** : intelligence artificielle générale capable de s’améliorer elle-même de façon autonome, en modifiant ses algorithmes, sa structure ou ses connaissances, pour accroître ses performances ou élargir ses capacités sans intervention humaine.
[^20]: **Traités de non-prolifération (TNP)** : accords internationaux visant à empêcher la diffusion et le développement de technologies ou d’armes jugées dangereuses (par exemple, le Traité sur la non-prolifération des armes nucléaires). Ils fixent des règles pour limiter l’accès à ces technologies, encourager le désarmement, et promouvoir leur usage exclusivement à des fins pacifiques, tout en mettant en place des mécanismes de contrôle et de vérification entre États signataires.
[^21]: <a href="https://ccdcoe.org/uploads/2020/12/Cyber-Threats-and-NATO-2030_Horizon-Scanning-and-Analysis.pdf" target="_blank">Cyber Threats and NATO 2030: Horizon Scanning and Analysis</a>
[^22]: <a href="https://artificialintelligenceact.eu/fr/" target="_blank">La loi européenne sur l'intelligence artificielle</a>
[^23]: <a href="https://web-archive-org.translate.goog/web/20231101123904/https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023?_x_tr_sl=en&_x_tr_tl=fr&_x_tr_hl=fr&_x_tr_pto=rq" target="_blank">The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023</a>
[^24]: <a href="https://www.un.org/fr/global-issues/artificial-intelligence#:~:text=À%20la%20recherche%20d'une,risques%20qui%20y%20sont%20associés." target="_blank">L'intelligence artificielle</a>
[^25]: <a href="https://www.nato.int/cps/en/natohq/official_texts_227237.htm" target="_blank">Summary of NATO's revised Artificial Intelligence (AI) strategy 10 July 2024</a>
[^26]: <a href="https://attack.mitre.org" target="_blank">MITRE ATT&CK</a>
