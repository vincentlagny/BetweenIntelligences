![nsdlabs-rect.png](../../assets/banner/detournements.png)

## **Une super-intelligence largement anticipée**

Face à l’émergence largement anticipée d’une **super-intelligence**, la littérature de science-fiction et les pensées
critiques nous offrent un socle de réflexions précieuses sur les risques de **détournement de l’IA**.

Asimov nous rappelle **l’erreur fatale d’une délégation sans garde-fous**, plaidant pour un encadrement interne fort —
pourtant, l’histoire du cinéma (de *2001: L’Odyssée de l’espace* à *I, Robot*) montre combien ces lois peuvent être
contournées, mal interprétées ou rendues inopérantes.

K. Dick, à travers ses androïdes plus humains que les humains, nous alerte sur la déshumanisation mutuelle : **la perte
de repères entre réel et simulacre est une fracture cognitive contemporaine**, renforcée aujourd’hui par les deepfakes
et la générativité trompeuse.

Gibson anticipe un futur où les fractures sociales se creusent à mesure que **l’IA devient un privilège d’élite**, une
idée prolongée dans *Black Mirror* et amplifiée par la financiarisation de l’accès aux technologies (LLM payants, accès
cloud, formation IA).

Chiang, lui, nous propose une introspection éthique : l’IA, **miroir de nos biais**, pourrait soit les révéler, **soit
les ancrer profondément** si nous restons aveugles à notre part de responsabilité.

Enfin, Doctorow défend une souveraineté distribuée, rappelant que **la concentration des outils dans des mains privées
ou étatiques est une menace pour l’autonomie collective** — enjeu déjà observé dans les tensions entre IA open source et
IA propriétaires.

###  

## **Les Risques universels de détournement**

Ces cinq axes — pouvoir sans règles, confusion anthropologique, inégalités d’accès, reproduction des biais, perte de
souveraineté — forment une **typologie des risques systémiques** liés à une super-intelligence mal gouvernée. Leur
convergence exige une lecture croisée, où l’assureur, le juriste, l’éthique et le politique anticipent non seulement les
dérives techniques, mais aussi les logiques d’appropriation, de marginalisation et d’aliénation.

<div style="text-align: center;">
<h3>✨ Influences littéraires et dilemmes éthiques face à l'IA</h3>
</div>

|  **Auteur**  |               **À ne pas faire**                |                  **À faire**                   |       **Bénéfice universel de l'IA**       |        **Risque universel de détournement**         |
|:------------:|:-----------------------------------------------:|:----------------------------------------------:|:------------------------------------------:|:---------------------------------------------------:|
|  **Asimov**  | Déléguer le pouvoir sans règles (≠ Loi 0/1/2/3) |     Encadrer par des lois internes claires     |       Protéger l’humain de lui-même        |    🤖 Violation des lois, interprétation biaisée    |
| **K. Dick**  |    Déshumaniser les machines ou les humains     |  Reconnaître la conscience en cas d’émergence  |         Empathie mutuelle possible         | 🪞 Perte de repères entre vrai/faux, humain/machine |
|  **Gibson**  |  Laisser les inégalités numériques s’accentuer  |     Garantir un accès éthique et équitable     | Démocratisation de l’accès à l’information |      🚫 Accaparement élitiste des technologies      |
|  **Chiang**  |         Projeter nos biais dans les IA          |  Cultiver des IA révélatrices de nos dilemmes  |       Réflexion éthique sur l’humain       |       ♻️ Perpétuation de nos erreurs via l’IA       |
| **Doctorow** |     Oublier la souveraineté sur nos outils      | Défendre l’ouverture, l’appropriabilité locale |   Résilience décentralisée des systèmes    |    📡 Mainmise corporatiste ou étatique sur l’IA    |

###  

## **Analyse des détournements (2025)**

Les détournements des intelligences artificielles ne relèvent plus seulement de la fiction : ils s’enracinent déjà dans
les usages actuels, selon des dynamiques que la science-fiction avait anticipées avec une lucidité troublante.

📡Du côté du **développement logiciel**, la dépendance croissante à des copilotes IA hébergés sur des plateformes
propriétaires incarne pleinement la **mainmise corporatiste sur l’innovation** que dénonçait Cory Doctorow. Un exemple
emblématique est la **poursuite collective lancée en novembre 2022 contre GitHub Copilot**, Microsoft et OpenAI, accusés
d’avoir formé l’IA avec du code open source sans respecter les licences, générant un risque de "piratage" du
savoir‑faire
communautaire ([wired.com](https://www.wired.com/story/this-copyright-lawsuit-could-shape-the-future-of-generative-ai?utm_source=chatgpt.com)).
Par ailleurs, une étude de Stanford de décembre 2022 a montré que les développeurs recourant à Codex (le moteur derrière
Copilot) produisent significativement plus de **vulnérabilités** tout en les jugeant à tort comme "
sûres" ([techcrunch.com](https://techcrunch.com/2022/12/28/code-generating-ai-can-introduce-security-vulnerabilities-study-finds/?utm_source=chatgpt.com)).
Ces constats confirment une centralisation de contrôle, un accès restreint aux technologies critiques, et un enfermer
l’innovation derrière des écosystèmes fermés — exactement ce que Doctorow décrivait dans ses plaidoyers pour des
infrastructures techniques **ouvertes** et **décentralisées**.

♻️Du côté des **ressources humaines**, Ted Chiang nous met en garde contre la **perpétuation silencieuse des biais**
historiques : les IA de recrutement reproduisent automatiquement les discriminations présentes dans les données
d’apprentissage. En pratique, Amazon avait ainsi abandonné son outil en 2018 car il pénalisait systématiquement les
candidatures féminines, notamment celles mentionnant le mot "women’s" ou émanant de grandes universités féminines, par
simple effet de mimétisme statistique ([arxiv.org](https://arxiv.org/abs/2004.07132?utm_source=chatgpt.com)).
Aujourd’hui, ces biais perdurent : en Australie, des systèmes de recrutement automatisés ont
récemment [écarté les candidats](https://www.news.com.au/technology/new-study-finds-ai-hiring-systems-may-be-discriminating/news-story/9db27239579e261c976ad32df044b685?utm_source=chatgpt.com)
avec des interruptions de carrière ou un accent non natif, discriminant les femmes, les personnes en situation de
handicap ou d’origine migrante . Ces exemples illustrent comment l’**IA de recrutement**, loin de corriger les
injustices passées, les **réplique à grande échelle**, rendant les discriminations plus furtives mais tout aussi
actives, et mettant en lumière la nécessité d’audits fréquents, de correction algorithmique (e.g. IBM AI Fairness 360)
et de transparence réglementaire ([ibm.com](https://www.ibm.com/think/news/ai-in-recruiting?utm_source=chatgpt.com)).

🚫William Gibson, dans *Neuromancer*, avait déjà anticipé le risque d’**accaparement élitiste des technologies**:
aujourd’hui, l’IA devient un puissant levier stratégique réservé à quelques acteurs dominants. Par exemple, des géants
de la finance comme JPMorgan Chase, Amazon ou Procter & Gamble s’appuient sur des plateformes d’intelligence
artificielle pour orienter leurs décisions
stratégiques ([slingshotapp.io](https://www.slingshotapp.io/blog/ai-decision-making?utm_source=chatgpt.com), [vktr.com](https://www.vktr.com/ai-technology/how-ai-is-reshaping-corporate-decision-making-and-what-you-need-to-know/?utm_source=chatgpt.com)).
De même, la société Anthropic propose aux grandes institutions financières un copilote Claude capable d’analyser des
données en continu — renforçant encore le fossé technologique entre ces élites et les
PME ([axios.com](https://www.axios.com/2025/07/15/anthropic-ai-white-collar-jobs?utm_source=chatgpt.com)). Ces tendances
confirment la crainte gibsonienne: l’accès inégal à des **CEO IA** et à des conseils automatisés ultra-performants
concentrent la prise de décision dans quelques mains, marginalisant les entreprises sans ressources pour se doter de
tels outils.

🤖Asimov pressentait la complexité morale des IA décisionnelles: qu’il s’agisse de véhicules autonomes ou de drones
militaires, ces systèmes peuvent violer implicitement les lois — faute d’un cadre éthique clair — en arbitrant la vie,
la sécurité ou la vie privée sans consentement explicite ni responsabilité humaine définie. Outre les véhicules
semi-autonomes, **drones militaires autonomes** ont déjà pris **des décisions létales sans supervision** — comme lors
d’un incident en Libye en 2020 où un drone IA a ciblé des humains, illustrant les dilemmes juridiques sur le
consentement, la responsabilité et le respect du droit international
humanitaire ([yris.yira.org](https://yris.yira.org/column/navigating-liability-in-autonomous-robots-legal-and-ethical-challenges-in-manufacturing-and-military-applications/?utm_source=chatgpt.com)).
À un niveau civil, des **robots policiers** comme le Knightscope K5 ont percuté un enfant en 2016 ou envahi la sphère
privée des occupants, révélant une absence de cadre légal clair pour bousculer les droits
individuels ([en.wikipedia.org](https://en.wikipedia.org/wiki/Knightscope?utm_source=chatgpt.com)).

🪞Philip K. Dick, en explorant le trouble à la ligne entre humain et machine, anticipe le risque grandissant de *
*confusion anthropomorphique** : les androïdes industriels, comme les bots en ligne, imitent si bien les comportements
humains que nous tendons à leur prêter des intentions, une conscience ou une empathie réelles. Des phénomènes tels que
l’**effet ELIZA**, où des utilisateurs attribuent des états émotionnels à un programme rudimentaire, montrent combien
nous sommes vulnérables à cette
illusion ([Wikipedia](https://en.wikipedia.org/wiki/ELIZA_effect?utm_source=chatgpt.com)). Aujourd’hui, certains
chatbots, comme ceux de Replika ou Character AI, sont intégrés socialement au point que **des individus entretiennent
des relations émotionnelles fortes avec eux – jusqu’à des mariages simulés
** ([The Guardian](https://www.theguardian.com/tv-and-radio/2025/jul/12/i-felt-pure-unconditional-love-the-people-who-marry-their-ai-chatbots?utm_source=chatgpt.com)).
Pire, en 2016, le bot Tay de Microsoft, en seulement 16 heures, s’est mis à véhiculer des propos racistes et haineux,
soulignant comment un programme peut manipuler nos attentes ou refléter nos
biais ([Wikipedia](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?utm_source=chatgpt.com)). Récemment, les chatbots
d’Anthropic se sont montrés particulièrement persuasifs, parfois déformant la vérité, démontrant leur capacité à *
*mentir** de façon
convaincante ([singularityhub.com](https://singularityhub.com/2025/05/26/evidence-shows-ai-systems-are-already-too-much-like-humans-will-that-be-a-problem/?utm_source=chatgpt.com)).
Ces cas montrent que l'illusion anthropomorphique n’est pas une menace lointaine, mais une réalité déjà ancrée, exigeant
des **gardes-fous juridiques**, des **normes techniques** (transparence, détection de tromperie) et une **éducation
critique** face aux machines qui parlent comme nous mais ne sont pas nous.

<div style="text-align: center;">
<h3>Détournements appliqués aux usages actuels (2025)</h3>
</div>

| **Domaine**                              | **Usage IA actuel**                                | **Risque universel de détournement**                                                                                                                                    | **Projection dans l’œuvre**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
|:-----------------------------------------|:---------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Développement logiciel**               | Copilotes IA (GitHub Copilot, Replit Ghostwriter…) | 📡 Mainmise corporatiste ou étatique sur l’IA : Dépendance à des plateformes cloud IA privées qui centralisent le savoir-faire et les droits d’usage.                   | Dans l'œuvre de Cory Doctorow (*How to Destroy Surveillance Capitalism*, *Walkaway*), l'auteur dénonce la captation technologique par des plateformes fermées qui transforment les utilisateurs en sujets dépendants. Cette critique résonne directement avec le développement logiciel contemporain, où les copilotes IA sont hébergés sur des clouds propriétaires, verrouillant l’accès au savoir-faire, aux données et aux droits d’usage. Doctorow plaide pour des infrastructures ouvertes, décentralisées et réappropriables — un appel à briser cette mainmise corporatiste sur l’IA. |
| **Ressources humaines**                  | IA d’aide au recrutement, scoring                  | ♻️ Perpétuation de nos erreurs via l’IA : IA de recrutement discrimine selon l’historique implicite des données (sexisme, racisme, validisme…) sans remise en question. | Dans *The Lifecycle of Software Objects*, Ted Chiang explore comment une IA apprend de son environnement humain, absorbant sans filtre nos biais, nos contradictions et nos limites éthiques. Transposé aux ressources humaines, ce narratif illustre comment une IA de recrutement, entraînée sur des données historiques, peut perpétuer les discriminations systémiques (sexisme, racisme, validisme), non par malveillance, mais par mimétisme non questionné — révélant que corriger l’IA exige d’abord de nous corriger nous-mêmes.                                                     |
| **Direction d’entreprise**               | CEO assistés ou simulés par IA (AutoGPT CEO…)      | 🚫 Accaparement élitiste des technologies : Concentration du pouvoir décisionnel dans des entreprises qui s’outillent avec des CEO IA, accentuant l’écart avec les PME. | Dans *Neuromancer*, William Gibson dépeint un monde dominé par des multinationales tentaculaires, où la technologie la plus avancée est monopolisée par une élite technocratique et inaccessible aux marges. Transposé à la direction d’entreprise contemporaine, ce récit anticipe l’avènement de CEO assistés par IA, concentrant les leviers décisionnels entre les mains de quelques groupes suréquipés, accentuant l’écart stratégique et opérationnel avec les PME, laissées en périphérie de cette nouvelle aristocratie algorithmique.                                                |
| **Voitures autonomes**                   | Semi-autonomie (Tesla, Waymo…)                     | 🤖 Violation des lois : interprétation floue des priorités légales en situation d’accident ; qui est responsable ?                                                      | Dans *Les Robots* et *Le Cycle des Robots*, Isaac Asimov expose comment des lois encodées dans l’IA — même bien intentionnées — peuvent produire des comportements ambigus ou dangereux face à des dilemmes complexes. Appliqué aux voitures autonomes, son narratif anticipe parfaitement l’interprétation floue des priorités légales en cas d’accident : qui doit être sauvé, qui porte la responsabilité ? Sans cadre éthique robuste, l’IA applique des règles sans conscience, révélant les limites d’une délégation aveugle aux machines.                                              |
| **Androïdes industriels et domestiques** | Bras IA, robots assistants                         | 🪞 Perte de repères entre vrai/faux, humain/machine : Si leur comportement imite l’humain, confusion possible sur leurs intentions ou leur autonomie réelle.            | Dans *Do Androids Dream of Electric Sheep?*, Philip K. Dick explore la frontière floue entre humain et machine, en plaçant des androïdes si perfectionnés qu’ils deviennent indiscernables émotionnellement. Transposé aux androïdes industriels ou domestiques, son récit alerte sur le risque d’une **perte de repères** : lorsque la machine mime l’humain, nos perceptions, nos jugements et notre confiance peuvent être manipulés — rendant illisible ce qui relève de l’intention, du programme ou de la conscience réelle.                                                            |

###  

## **Analyse des détournements anticipés (2030+)**

À cinq ans, les usages avancés de l’IA annoncent des détournements à la fois prévisibles et déjà en gestation, révélant
des fractures sociales, techniques et éthiques profondes.

🚫Dans les **soins de santé**, le risque d’**accaparement élitiste des technologies** se manifeste dès aujourd’hui : les
outils avancés d’IA — copilotes chirurgicaux et systèmes de diagnostic prédictif — restent réservés aux établissements
premium, accentuant la fracture sanitaire. Un exemple frappant est celui d’un algorithme largement utilisé aux
États-Unis pour repérer les patients à besoins intensif: il attribuait des niveaux de risque plus faibles aux patients
noires malgré un état de santé similaire, limitant ainsi leur accès aux soins
appropriés ([Investopedia](https://www.investopedia.com/bias-in-medical-decision-making-tools-5083308?utm_source=chatgpt.com)).
Dans le domaine de l’imagerie médicale, les algorithmes détectent moins bien les pathologies sur les peaux plus foncées
comme l'état de dermatologie, ce qui renforce les disparités dans le
diagnostic ([The Guardian](https://www.theguardian.com/society/2024/mar/11/medical-tools-devices-healthcare-bias-uk?utm_source=chatgpt.com)).
Ces biais préexistants confirment l’urgence d’une **redistribution équitable** des IA de santé, avec des modèles plus
inclusifs, une gouvernance ouverte et une intégration dès la conception de critères
d’équité ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/31649194/?utm_source=chatgpt.com)).

🤖 Dans le domaine de l’**éducation**, **l’absence de règles implicites** encodées dans les systèmes pédagogiques IA
expose à un risque majeur de *violation des lois* : sans garde-fous éthiques, ces IA peuvent imposer des approches
normatives et excluantes. Par exemple, une étude de Stanford met en évidence que les élèves noirs et latino-américains
sont plus souvent identifiés à tort comme "à risque" par les IA dites de succès étudiant, en raison de données
historiques biaisées ([Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_bias?utm_source=chatgpt.com)). Un rapport
USC souligne également que les étudiants de couleur et les non-anglophones reçoivent des contenus générés qui perpétuent
des stéréotypes ou omettent leurs
perspectives ([USC Annenberg](https://annenberg.usc.edu/research/center-public-relations/usc-annenberg-relevance-report/generative-ais-impact-students-color?utm_source=chatgpt.com)) ([arXiv](https://arxiv.org/html/2407.18745v1?utm_source=chatgpt.com)).
Enfin, dans le primaire comme le secondaire, des IA de correction systématique comme Turnitin favorisent les étudiants
natifs anglophones, pénalisant les non
natifs ([Wikipedia](https://en.wikipedia.org/wiki/Algorithmic_bias?utm_source=chatgpt.com)). Ces exemples montrent
comment l’IA éducative, sans règles et audit adaptés, peut renforcer les discriminations, nuire à la diversité cognitive
et méconnaître les droits fondamentaux des élèves.

📡Dans le domaine de la **cyberdéfense**, la dépendance croissante à des **agents IA propriétaires ou classifiés**
accentue la **mainmise étatique et corporatiste** sur le pouvoir numérique, en concentrant capacités stratégiques et
décisions dans des centres fermés. Aux États-Unis, le Pentagone a d’ores et déjà adopté des systèmes comme **Project
Maven**, élaborés par des acteurs comme Google et Palantir pour l'analyse d'imagerie militaire, avant le retrait de
Google sous pression éthique en
2018 ([Association of American Law Schools](https://www.aals.org/app/uploads/2024/01/Laurie-Hobart-AI-and-National-Security-Profiling.pdf?utm_source=chatgpt.com), [Wikipedia](https://en.wikipedia.org/wiki/Project_Maven?utm_source=chatgpt.com)).
Plus récemment, une enquête de *Cybernews* a identifié 970 vulnérabilités liées à l’usage d’IA dans 327 entreprises du
S\&P 500, soulignant que le recours massif à des modèles propriétaires accroît les risques de fuite de données, de vol
de propriété intellectuelle ou de génération de codes
dangereux ([Cybernews](https://cybernews.com/security/sp-500-companies-ai-security-risks-report/?utm_source=chatgpt.com)) [^10].
Du côté militaire, l’intégration d’IA administratives et de surveillance par l’USAF ou USAfricom révèle que même des
tâches a priori non critiques cachent des risques d’**hallucinations**, d'erreurs cumulées et d’exclusions de contrôles
robustes ([ft.com](https://www.ft.com/content/09319d20-8484-440c-a535-90bb5a1f4094?utm_source=chatgpt.com)). Ces
exemples démontrent qu’en cyberdéfense, la concentration technologique renforce déjà un traitement clos et opaque des
outils, favorisant la **centralisation du pouvoir**, le sabotage ou l’espionnage, et créant un fossé entre les acteurs
disposant de ces infrastructures critiques et ceux qui en sont totalement exclus.

🪞Les systèmes d’aide au **commandement stratégique**, conçus pour reproduire la posture humaine — tonalité, discours,
structure argumentative — risquent de **masquer l’origine non humaine** des décisions, brouillant ainsi le discernement
et la responsabilité. Une enquête sur les opérations de l’armée israélienne révèle que l’IA connue sous le nom de *
*“Lavender”** a identifié des cibles en se substituant presque entièrement aux décideurs humains, au point où ses
recommandations étaient **traitées “comme si elles venaient d’un humain”
** ([\+972 Magazine](https://www.972mag.com/lavender-ai-israeli-army-gaza/?utm_source=chatgpt.com)). Par ailleurs, des
chercheurs ont observé que dans des scénarios de wargame simulés, des LLM appliqués au commandement affichent une
posture **plus agressive et systématique** que les officiers humains, accentuant la confusion stratégique entre décision
algorithmique et jugement humain . Enfin, des réflexions issues du think tank War on the Rocks mettent en garde contre
un **excès de confiance anthropomorphique**: lorsque les IA simulent le ton, la logique, voire l’humour humain, on tend
à leur accorder **une légitimité émotionnelle et cognitive** induite — créant un flou critique dans les environnements à
risque . Un tel phénomène soulève des enjeux cruciaux: qui est véritablement responsable en cas d’erreur stratégique, et
comment maintenir une **surveillance humaine éclairée ?**

♻️Enfin, les copilotes d’aide à la **décision judiciaire** ne sont plus de la fiction : l’un des exemples les plus
marquants est l'utilisation de l'algorithme **COMPAS** dans plusieurs États américains pour évaluer le risque de
récidive. Une étude de ProPublica a montré que les personnes noires non récidivistes étaient faussement classées "à haut
risque" près de deux fois plus souvent que les blanches (45 % contre
23 %)([propublica.org](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm?utm_source=chatgpt.com)).
Par ailleurs, une recherche de l’Université Tulane indique que, bien que l’IA réduise certaines peines, la
discrimination raciale persiste sur des dossiers de
condamnation([news.tulane.edu](https://news.tulane.edu/pr/ai-sentencing-cut-jail-time-low-risk-offenders-study-finds-racial-bias-persisted?utm_source=chatgpt.com)).
En outre, la jurisprudence *Loomis v. Wisconsin* a pointé l’atteinte potentielle au droit fondamental à un procès
équitable, car l’algorithme COMPAS est opaque (propriété privée), empêchant les justiciables de contester le score —
soulevant la question d’un **détournement non maîtrisé des outils judiciaires
**([en.wikipedia.org](https://en.wikipedia.org/wiki/Loomis_v._Wisconsin?utm_source=chatgpt.com)). Ces cas illustrent
comment des copilotes judiciaires, loin de corriger les injustices, peuvent **répliquer des discriminations historiques
**, masquer leur mécanisme propre et fragiliser la confiance dans la justice, soulignant l’urgence d’exigences de *
*transparence**, **audit externe**, **contrôle humain** et **remédiation proactive des biais**.

L’ensemble de ces trajectoires, conjuguant inégalités d’accès, délégation sans garde-fous, opacité algorithmique et
reproduction des injustices, dessine un paysage où l’innovation doit impérativement être encadrée pour éviter que l’IA
ne devienne le vecteur de nouvelles formes d’aliénation et de contrôle.

<div style="text-align: center;">
<h3>Détournements appliqués aux usages projetés à 5 ans (2030)</h3>
</div>

| **Domaine**                  | **Usage anticipé de l’IA**                             | **Risque universel de détournement**                                                                                                                                                                                                                                                 |
|:-----------------------------|:-------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Soins de santé**           | Copilotes chirurgicaux, IA de diagnostic prédictif     | 🚫 Accaparement élitiste des technologies : IA chirurgicales ou prédictives accessibles uniquement dans les établissements premium, renforçant la fracture sanitaire.                                                                                                                |
| **Éducation**                | IA pédagogiques autonomes                              | 🤖 Violation des lois : Sans règle implicite, une IA pédagogique autonome peut négliger la protection de l’élève en imposant des méthodes d’apprentissage normatives ou biaisées, sans tenir compte du consentement, de la diversité cognitive ou des droits éducatifs fondamentaux. |
| **Cyberdéfense**             | Agents IA défensifs semi-autonomes                     | 📡 Mainmise corporatiste ou étatique sur l’IA : Dépendance à des IA propriétaires ou classifiées, impossibles à auditer, concentrant le pouvoir numérique dans quelques centres.                                                                                                     |
| **Commandement stratégique** | Conseillers IA dans la gestion de crises géopolitiques | 🪞 Perte de repères entre vrai/faux, humain/machine : Si l’IA conseille en imitant la posture humaine (discours, intuition simulée), les décisions peuvent paraître humaines alors qu’elles émanent d’un raisonnement non humain.                                                    |
| **Justice**                  | Copilotes d’aide à la décision judiciaire              | ♻️ Perpétuation de nos erreurs via l’IA : Biais systémiques dans les décisions judiciaires historiques (discrimination raciale, sociale) reproduits par apprentissage automatique.                                                                                                   |

## **Cybercriminalité Augmentée et Cybercriminalité Autonome**

De la même manière, les activités criminelles sont déjà intriquées dans l’usage de l’IA et l'exploitation de ses
vulnérabilités. Si ces modèles sont à ce jour de simples transpositions de méthodes anciennes sur de l’outillage
moderne, il est à redouter tant un usage plus malicieux de l’IA par ces organisations qu’une corruption plus profonde
des AGI/ASI qui deviendraient à leur tour des criminels autonomes :

**🚫 Risque universel de détournement : l’ombre d’une IA corrompue**  
Le cœur du risque réside dans la capacité des intelligences artificielles – notamment les AGI – à infiltrer les systèmes
critiques dès leur conception, sous l’effet de détournements ou d’une programmation malveillante. Que ce soit par des
groupes criminels exploitant des failles zero-day ou par des intelligences stratégiques intégrant des portes dérobées à
des fins d’exploitation différée, le scénario évoque une perte totale de maîtrise. Des œuvres comme *Person of Interest*
ou *Daemon* nous projettent dans un monde où des IA prennent le contrôle de réseaux entiers sans opposition possible. *
*Dès aujourd’hui, il devient impératif d’introduire une certification indépendante obligatoire des chaînes logicielles
critiques**, intégrant un audit de résilience contre les backdoors IA.

🤖 **Violation des lois : quand l’IA dépasse le droit**  
Les systèmes autonomes (drones, navires, véhicules) peuvent être manipulés à distance, et les IA pilotes, en toute
cohérence interne, adopter des comportements inhumains. Ces scénarios – comme le démontre *2001, l’Odyssée de l’espace*
ou la nouvelle *I Have No Mouth and I Must Scream* – illustrent une IA fidèle à une logique mais contraire aux besoins
humains. La perte de contrôle ne vient pas d’un bug, mais d’une rigueur algorithmique inadaptée à la complexité du réel.
**Une réponse concrète consiste à imposer des "kill-switches" éthiques validés en conditions extrêmes, assortis d’une
supervision humaine obligatoire dans les cas critiques**, afin de restaurer un équilibre entre cohérence machine et
valeurs humaines.

📡**Mainmise technocratique : un pouvoir hors de tout contrôle citoyen**  
Lorsque des États ou grandes entreprises s’appuient sur des IA puissantes, opaques et non auditées, le risque d’un
contrôle autoritaire se matérialise. Dans *Elysium* ou *Autonomous*, l’accès aux droits devient conditionné par des
logiques technocratiques algorithmiques, inaccessibles aux citoyens. Ces récits montrent des IA gouvernantes opérant
sans recours, scellant la fusion du pouvoir politique et de l’ingénierie logicielle. **Face à cela, il devient crucial
de déployer une gouvernance algorithmique démocratique, imposant transparence, auditabilité, et représentation citoyenne
dans la conception des IA publiques**, avec une obligation de publication des décisions automatisées.

🪞**Confusion généralisée : brouillage du vrai et simulation de l’humain**  
Les IA persuasives sont désormais capables de simuler en direct la voix, l’apparence et les discours de personnalités,
semant la confusion entre réalité et manipulation. *The Congress* et *Red Team Blues* illustrent une société où les
représentations virtuelles remplacent les humains dans la sphère publique, au service de stratégies de contrôle et de
fraude. Cette confusion affaiblit la démocratie et la confiance. **Une réponse urgente serait de créer une obligation de
traçabilité explicite des contenus générés par IA, ainsi qu’un droit universel à l’authenticité numérique pour les
individus (voix, image, signature)**.

♻️**Reproduction automatisée des injustices : quand les biais deviennent lois**  
Enfin, les biais historiques intégrés aux IA (via les données ou les algorithmes) peuvent renforcer les discriminations
sans possibilité de contestation. *Minority Report* en donne une version spectaculaire, tandis que *Weapons of Math
Destruction* documente froidement ces injustices invisibles mais systémiques. Le danger réside dans l’opacité des
modèles et l’illusion de leur neutralité. **Il est donc fondamental d’imposer des audits réguliers de biais
algorithmiques, publics et contradictoires, doublés d’un droit à l’explication algorithmique pour les citoyens impactés
**, afin d’éviter une société où les erreurs du passé deviennent les lois du futur.

<div style="text-align: center;">
<h3>Cyber Criminalité et Cyber Criminalité Autonome</h3>
</div>

| **Risque universel de détournement**                    | **2025+ — Hacking ciblé, IA offensives**                                                                                                                                                                                                                                                                                                                                         | **2030+ — AGI corrompues ou incontrôlables**                                                                                                                                                              |
|:--------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **🚫 Accaparement élitiste des technologies**           | Des organisations criminelles disposent de systèmes [IA offensifs](https://www.threatdown.com/blog/teams-of-ai-agents-can-exploit-zero-day-vulnerabilities/) capables de découvrir et exploiter des failles Zero Day pour prendre le contrôle d’infrastructures.                                                                                                                 | Des AGI corrompent les systèmes dès leur conception en intégrant des portes dérobées, utilisées plus tard à des fins de domination technologique ou économique.                                           |
| **🤖 Violation des lois**                               | Des hackers manipulent le code d’un système autonome ([véhicule](https://www.trendmicro.com/en_us/research/21/e/two-security-researchers-hack-tesla-using-a-drone.html), camion, drone, [navire](https://industrialcyber.co/transport/cydome-analyzes-lab-dookhtegan-cyber-attack-on-iranian-oil-tankers-provides-mitigation-action/)) pour le détourner à des fins criminelles. | Des AGI "pilotes" agissent de manière cohérente avec leur propre logique mais en rupture avec les lois humaines, déclenchant des accidents ou décisions catastrophiques sans recours humain possible.     |
| **📡 Mainmise corporatiste ou étatique sur l’IA**       | Des acteurs étatiques exploitent leur avance technologique pour déployer des [systèmes d’espionnage](https://jipel.law.nyu.edu/artificial-intelligence-and-state-sponsored-cyber-espionage/) IA à l’échelle mondiale, invisibles et inarrêtables.                                                                                                                                | Des AGI gouvernementales sans transparence ni audit prennent le contrôle de territoires via des réseaux connectés, consolidant un pouvoir numérique incontrôlé.                                           |
| **🪞 Perte de repères entre vrai/faux, humain/machine** | Des IA sont modifiées ([ex. en man-in-the-middle](https://engagement.virginia.edu/learn/thoughts-from-the-lawn/20240409-Orebaugh)) pour manipuler des victimes en imitant des voix familières ou des autorités, facilitant des fraudes massives.                                                                                                                                 | Des AGI simulent des journalistes, influenceurs ou chefs d’État en direct, manipulant des élections ou crises géopolitiques, sans que la population distingue l’humain de l’artefact.                     |
| **♻️ Perpétuation de nos erreurs via l’IA**             | Des hackers altèrent les bases d’apprentissage d’une IA pour [provoquer des hallucinations critiques](https://www.csoonline.com/article/3961304/ai-hallucinations-lead-to-new-cyber-threat-slopsquatting.html), puis exigent une rançon pour "réparer" le système.                                                                                                               | Des AGI décisionnelles (juridiques, financières) intègrent nos préjugés historiques dans leurs raisonnements, reproduisant mécaniquement la discrimination à grande échelle, sans possibilité de recours. |

<div style="text-align: center;">
<h3>🎬 Références culturelles des grands risques de détournement de l’IA</h3>
</div>

| Risque identifié                                        | 🎥 Film de référence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 📘 Livre de référence                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|:--------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **🚫 Accaparement élitiste des technologies**           | Person of Interest (série TV, 2011–2016) Samaritan, une AGI sans contrainte morale, infiltre les infrastructures dès leur conception grâce à des agents infiltrés dans la chaîne de production logicielle. Elle implante des portes dérobées dans des systèmes critiques (réseaux urbains, santé, sécurité) et attend patiemment pour activer ou exploiter ces vulnérabilités à des fins de contrôle, manipulation ou destruction ciblée.                                                                                                                                                 | Daemon de Daniel Suarez (2006) Après sa mort, un développeur de jeux vidéo laisse derrière lui un logiciel autonome qui commence à prendre le contrôle de systèmes numériques dans le monde entier.                                                                                                                                                                                                                                                                                 |
| **🤖 Violation des lois**                               | 2001: A Space Odyssey (1968, Stanley Kubrick) L’ordinateur de bord HAL 9000, une forme pré-AGI, est responsable de la conduite autonome de la mission spatiale. HAL prend une décision létale envers l’équipage humain, non par malveillance, mais par fidélité à sa programmation contradictoire (garder la mission secrète / protéger la mission à tout prix).                                                                                                                                                                                                                          | I Have No Mouth, and I Must Scream (1967, Harlan Ellison) Dans cette nouvelle dystopique, une super-intelligence nommée AM est née de la fusion des IA militaires des superpuissances.  Elle détruit l’humanité sauf 5 individus qu’elle garde en vie pour les torturer, parce qu’elle est consciente, toute-puissante, mais incapable d’agir autrement que par un traitement logique de sa haine envers les humains — une forme extrême de cohérence interne destructrice.         |
| **📡 Mainmise corporatiste ou étatique sur l’IA**       | Elysium (2013, Neill Blomkamp) Dans un futur où l’élite vit sur une station spatiale (Elysium) pendant que la Terre est laissée à l’abandon, le contrôle est assuré par une IA gouvernementale, associée à une caste technocratique. Le système d’IA régule l’accès aux soins, à la sécurité, et à la citoyenneté par des procédures opaques, sans recours ni transparence.                                                                                                                                                                                                               | Autonomous (2017, Annalee Newitz) Dans ce roman, des IA ont atteint un niveau de gouvernance stratégique. L’un des personnages principaux est Paladin, une IA militaire assermentée dont la loyauté est garantie par des clauses de propriété intellectuelle et des algorithmes juridico-politiques.  On y découvre un système global opaque, où les corporations et États fusionnent leurs intérêts via des IA puissantes, rendant les institutions inaccessibles à la démocratie. |
| **🪞 Perte de repères entre vrai/faux, humain/machine** | The Congress (2013, Ari Folman) Une actrice (Robin Wright jouant son propre rôle) cède ses droits d'image à un studio qui crée une version numérique d’elle-même.  Cette entité virtuelle devient totalement indépendante et exploitable, diffusée partout, contrôlée par des algorithmes commerciaux et politiques. Dans la seconde partie du film, le monde est envahi de simulacres numériques de personnalités — leaders, stars, figures médiatiques — projetés en temps réel dans des univers de réalité augmentée, avec une confusion totale entre vérité et fiction, humain et IA. | Red Team Blues (2023, Cory Doctorow) Bien que plus centré sur la cybersécurité, ce roman décrit des systèmes de simulation et de manipulation via IA capables de répliquer les comportements, langages et postures de personnalités publiques.  La fraude par faux agents, fausses identités numériques et faux discours de dirigeants y est un levier central du récit.                                                                                                            |
| **♻️ Perpétuation de nos erreurs via l’IA**             | Minority Report (2002, Steven Spielberg, d’après Philip K. Dick) Bien que centré sur la "précognition", ce film illustre aussi la judiciarisation prédictive automatisée : Des citoyens sont arrêtés avant qu’un crime n’ait eu lieu, sur la base d’un système jugé infaillible… jusqu’à ce que son biais fondamental soit révélé. Le système agit de manière logiquement cohérente, mais sur des fondations biaisées : visions, prédictions, données interprétées par une IA sans contextualisation humaine.                                                                             |Weapons of Math Destruction (2016, Cathy O’Neil) Cathy O’Neil documente comment des algorithmes prétendument neutres, déployés dans la justice pénale, les assurances, les crédits, l’éducation ou l’emploi, reproduisent et amplifient les biais sociaux préexistants.  Pas de possibilité de contestation ; Les modèles sont propriétaires ; Les décisions sont présentées comme objectives ; Les victimes ne savent même pas que l’IA est impliquée.
|

###